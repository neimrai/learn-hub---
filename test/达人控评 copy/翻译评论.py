# ç¿»è¯‘è¯„è®º
import requests
import time
import re
import json
from xbot import print
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def translate_text(query, to_language="zh", max_retries=3):
    """
    ä½¿ç”¨ç™¾åº¦ç¿»è¯‘ API ç¿»è¯‘æ–‡æœ¬ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶ã€‚
    
    å‚æ•°:
        query (str): å¾…ç¿»è¯‘çš„æ–‡æœ¬ã€‚
        to_language (str): ç¿»è¯‘ç›®æ ‡è¯­è¨€ï¼Œé»˜è®¤ä¸ºä¸­æ–‡ ("zh")ã€‚
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 3ã€‚
    
    è¿”å›:
        str: ç¿»è¯‘åçš„æ–‡æœ¬ã€‚å¦‚æœå‡ºé”™æˆ–å¤±è´¥ï¼Œè¿”å› "ç¿»è¯‘å¤±è´¥"ã€‚
    """

    # è¯·æ±‚å¤´
    headers = {
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Acs-Token": "1736762596345_1736824772122_QbhbgM2VDjM2+iNrnxAIqdQrKMVO5yRKl8irUixiOYZ9vyc14qnYJYQlw0t1/x27pL4NlhJczqr6gwz62fnLnXgOkAhFt44mu739pgnc/3hNWMzQfjz6GbUhryNask9Hj5pUokUPCWTpmjVAwqTS25Z91I585+LztQKBq8YALEoDoH31d+z6kKZEQF1geXebvvh8Y47XadTeQCTOH1zj8Nbez8+CslJ8we0K8BmoFmrxsQ1KXxteLXuxfOvsrWHhMnV5EFqZvmF0NRrsmOxAL2hDGV6c/qNy4zTSK8cVcaURyrZoEBBXLGEHSG+WvQ4oxgQ7EYBFN0MZT9vLkB/BZXZbcxzDBnnF7aKBmPIu8gWNnNPiqv+rixq5yG4Ac035u1/3PTK4UkNvXlQ/RQWbo8ilGy8uRDXeXZedwhQJaR/WCoIiYpxkZWNbagXTiCRojj2Sgmx9v/n1HflYF2m0NjqrKk2MCvIt+4cwFqP9CEwKJe9CB+Lvf5jywCwV0EB9",
        "Connection": "keep-alive",
        "Content-Type": "application/json",
        "Origin": "https://fanyi.baidu.com",
        "Referer": "https://fanyi.baidu.com/mtpe-individual/multimodal",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "accept": "text/event-stream",
        "sec-ch-ua": "\\Google",
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": "\\Windows"
    }

    # æ£€æµ‹è¯­è¨€çš„ URL
    lang_detect_url = "https://fanyi.baidu.com/langdetect"

    # ç¿»è¯‘çš„ URL
    translate_url = "https://fanyi.baidu.com/ait/text/translate"

    # å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’çº§ï¼‰
    milli_timestamp = str(int(time.time() * 1000))

    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # æ£€æµ‹è¯­è¨€
            if not query:
              return ''
            lang_detect_data = {"query": query}
            lang_response = requests.post(lang_detect_url, headers=headers, json=lang_detect_data).json()
            lan = lang_response.get("lan", None)  # è·å–æ£€æµ‹åˆ°çš„è¯­è¨€
            if not lan:
                print(f"è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œè¿”å›å†…å®¹: {lang_response}")
                continue  # é‡è¯•

            # ç¿»è¯‘æ•°æ®
            translate_data = {
                "query": query,
                "from": lan,
                "to": to_language,
                "reference": "",
                "corpusIds": [],
                "needPhonetic": False,
                "domain": "common",
                "milliTimestamp": int(milli_timestamp)
            }

            # å‘èµ·ç¿»è¯‘è¯·æ±‚
            translate_response = requests.post(translate_url, headers=headers, json=translate_data).text

            # ä»å“åº”ä¸­æå–ç¿»è¯‘ç»“æœ
            match = re.search(r'"message":"ç¿»è¯‘ä¸­","list":(.*?)]', translate_response)
            if match:
                rep = match.group(1) + "]"  # è¡¥å…¨ JSON æ•°ç»„
                rep = json.loads(rep)
                if rep and isinstance(rep, list) and "dst" in rep[0]:
                    return rep[0]["dst"]  # è¿”å›ç¿»è¯‘åçš„æ–‡æœ¬

            print(f"ç¬¬ {attempt + 1} æ¬¡ç¿»è¯‘å¤±è´¥ï¼Œè¿”å›å†…å®¹: {translate_response}")
        except Exception as e:
            print(f"ç¬¬ {attempt + 1} æ¬¡ç¿»è¯‘å‡ºé”™: {e}")

        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        time.sleep(1)

    # å¦‚æœæ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œè¿”å›ç¿»è¯‘å¤±è´¥çš„æ–‡æœ¬
    return "ç¿»è¯‘å¤±è´¥"

# å®šä¹‰æƒ…æ„Ÿåˆ†ç±»å‡½æ•°
def classify_sentiment(score):
    if score > 0.05:
        return 'æ­£é¢'
    elif score < -0.05:
        return 'è´Ÿé¢'
    else:
        return 'ä¸­æ€§'
      
# å®šä¹‰æƒ…æ„Ÿåˆ†æå‡½æ•°
def senti(text):
    return analyzer.polarity_scores(text)['compound']
  
  
# web_data_table =[['à¸›à¸­à¸™à¸”à¹Œà¸à¸£à¸°à¹€à¸šà¸·à¹‰à¸­à¸‡à¹à¸•à¸', 'à¹à¸›à¸¥à¸à¹ƒà¸«à¸¡à¹ˆà¸¡à¸²à¸ğŸ¥°', 1]]

def is_question(text):
  a = re.search(r'\b(what|who|where|why|how)\b', text, re.IGNORECASE) is not None or '?' in text
  if a:
    return 'æ˜¯'
  return 'å¦'

def convert_to_int(like_str):
    if like_str[-1].lower() == 'k':  # å¦‚æœä»¥ 'K' ç»“å°¾
        return int(float(like_str[:-1]) * 1000)
    elif like_str[-1].lower() == 'm':  # å¦‚æœä»¥ 'M' ç»“å°¾
        return int(float(like_str[:-1]) * 1000000)
    else:  # æ™®é€šæ•°å­—
        return int(like_str)
# åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
analyzer = SentimentIntensityAnalyzer()  


# web_data_table = [['à¸›à¸­à¸™à¸”à¹Œà¸à¸£à¸°à¹€à¸šà¸·à¹‰à¸­à¸‡à¹à¸•à¸', 'à¹à¸›à¸¥à¸à¹ƒà¸«à¸¡à¹ˆà¸¡à¸²à¸ğŸ¥°', 1]]
for row in web_data_table:
    row[2] = convert_to_int(row[2])
    row.append(translate_text(row[1], to_language="zh"))  # ç¿»è¯‘ä¸ºä¸­æ–‡
    
    translated_en = translate_text(row[1], to_language="en")  # ç¿»è¯‘ä¸ºè‹±æ–‡
    row.append(translated_en)
    if translated_en:
      # å¯¹ç¿»è¯‘åçš„è‹±æ–‡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æå’Œåˆ†ç±»
      sentiment_score_en = senti(translated_en)
      sentiment_category_en = classify_sentiment(sentiment_score_en)
      row.append(sentiment_category_en)  # æ·»åŠ è‹±æ–‡æƒ…æ„Ÿåˆ†ç±»ç»“æœ
      
      # å¯¹ç¿»è¯‘åçš„è‹±æ–‡æ–‡æœ¬è¿›è¡Œæé—®åˆ¤æ–­
      q = is_question(translated_en)
      row.append(q)  


print(web_data_table)
